[
  {
    "instance_id": "sqlfluff__sqlfluff-1625",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/src/sqlfluff/rules/L031.py b/src/sqlfluff/rules/L031.py\n--- a/src/sqlfluff/rules/L031.py\n+++ b/src/sqlfluff/rules/L031.py\n@@ -211,7 +211,7 @@ def _lint_aliases_in_join(\n             violation_buff.append(\n                 LintResult(\n                     anchor=alias_info.alias_identifier_ref,\n-                    description=\"Avoid using aliases in join condition\",\n+                    description=\"Avoid aliases in from clauses and join conditions.\",\n                     fixes=fixes,\n                 )\n             )\n"
  },
  {
    "instance_id": "sqlfluff__sqlfluff-2419",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/src/sqlfluff/rules/L060.py b/src/sqlfluff/rules/L060.py\n--- a/src/sqlfluff/rules/L060.py\n+++ b/src/sqlfluff/rules/L060.py\n@@ -59,4 +59,8 @@ def _eval(self, context: RuleContext) -> Optional[LintResult]:\n             ],\n         )\n \n-        return LintResult(context.segment, [fix])\n+        return LintResult(\n+            anchor=context.segment,\n+            fixes=[fix],\n+            description=f\"Use 'COALESCE' instead of '{context.segment.raw_upper}'.\",\n+        )\n"
  },
  {
    "instance_id": "sqlfluff__sqlfluff-1733",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/src/sqlfluff/rules/L039.py b/src/sqlfluff/rules/L039.py\n--- a/src/sqlfluff/rules/L039.py\n+++ b/src/sqlfluff/rules/L039.py\n@@ -44,7 +44,9 @@ def _eval(self, context: RuleContext) -> Optional[List[LintResult]]:\n                 # This is to avoid indents\n                 if not prev_newline:\n                     prev_whitespace = seg\n-                prev_newline = False\n+                # We won't set prev_newline to False, just for whitespace\n+                # in case there's multiple indents, inserted by other rule\n+                # fixes (see #1713)\n             elif seg.is_type(\"comment\"):\n                 prev_newline = False\n                 prev_whitespace = None\n"
  },
  {
    "instance_id": "sqlfluff__sqlfluff-1517",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/src/sqlfluff/core/parser/helpers.py b/src/sqlfluff/core/parser/helpers.py\n--- a/src/sqlfluff/core/parser/helpers.py\n+++ b/src/sqlfluff/core/parser/helpers.py\n@@ -2,6 +2,7 @@\n \n from typing import Tuple, List, Any, Iterator, TYPE_CHECKING\n \n+from sqlfluff.core.errors import SQLParseError\n from sqlfluff.core.string_helpers import curtail_string\n \n if TYPE_CHECKING:\n@@ -26,11 +27,11 @@ def check_still_complete(\n     \"\"\"Check that the segments in are the same as the segments out.\"\"\"\n     initial_str = join_segments_raw(segments_in)\n     current_str = join_segments_raw(matched_segments + unmatched_segments)\n-    if initial_str != current_str:  # pragma: no cover\n-        raise RuntimeError(\n-            \"Dropped elements in sequence matching! {!r} != {!r}\".format(\n-                initial_str, current_str\n-            )\n+\n+    if initial_str != current_str:\n+        raise SQLParseError(\n+            f\"Could not parse: {current_str}\",\n+            segment=unmatched_segments[0],\n         )\n     return True\n \n"
  },
  {
    "instance_id": "sqlfluff__sqlfluff-1763",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/src/sqlfluff/core/linter/linted_file.py b/src/sqlfluff/core/linter/linted_file.py\n--- a/src/sqlfluff/core/linter/linted_file.py\n+++ b/src/sqlfluff/core/linter/linted_file.py\n@@ -7,6 +7,8 @@\n \n import os\n import logging\n+import shutil\n+import tempfile\n from typing import (\n     Any,\n     Iterable,\n@@ -493,7 +495,24 @@ def persist_tree(self, suffix: str = \"\") -> bool:\n             if suffix:\n                 root, ext = os.path.splitext(fname)\n                 fname = root + suffix + ext\n-            # Actually write the file.\n-            with open(fname, \"w\", encoding=self.encoding) as f:\n-                f.write(write_buff)\n+            self._safe_create_replace_file(fname, write_buff, self.encoding)\n         return success\n+\n+    @staticmethod\n+    def _safe_create_replace_file(fname, write_buff, encoding):\n+        # Write to a temporary file first, so in case of encoding or other\n+        # issues, we don't delete or corrupt the user's existing file.\n+        dirname, basename = os.path.split(fname)\n+        with tempfile.NamedTemporaryFile(\n+            mode=\"w\",\n+            encoding=encoding,\n+            prefix=basename,\n+            dir=dirname,\n+            suffix=os.path.splitext(fname)[1],\n+            delete=False,\n+        ) as tmp:\n+            tmp.file.write(write_buff)\n+            tmp.flush()\n+            os.fsync(tmp.fileno())\n+        # Once the temp file is safely written, replace the existing file.\n+        shutil.move(tmp.name, fname)\n"
  },
  {
    "instance_id": "marshmallow-code__marshmallow-1359",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/src/marshmallow/fields.py b/src/marshmallow/fields.py\n--- a/src/marshmallow/fields.py\n+++ b/src/marshmallow/fields.py\n@@ -1114,7 +1114,7 @@ def _bind_to_schema(self, field_name, schema):\n         super()._bind_to_schema(field_name, schema)\n         self.format = (\n             self.format\n-            or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)\n+            or getattr(self.root.opts, self.SCHEMA_OPTS_VAR_NAME)\n             or self.DEFAULT_FORMAT\n         )\n \n"
  },
  {
    "instance_id": "marshmallow-code__marshmallow-1343",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/src/marshmallow/schema.py b/src/marshmallow/schema.py\n--- a/src/marshmallow/schema.py\n+++ b/src/marshmallow/schema.py\n@@ -877,7 +877,7 @@ def _invoke_field_validators(self, unmarshal, data, many):\n                 for idx, item in enumerate(data):\n                     try:\n                         value = item[field_obj.attribute or field_name]\n-                    except KeyError:\n+                    except (KeyError, TypeError):\n                         pass\n                     else:\n                         validated_value = unmarshal.call_and_store(\n@@ -892,7 +892,7 @@ def _invoke_field_validators(self, unmarshal, data, many):\n             else:\n                 try:\n                     value = data[field_obj.attribute or field_name]\n-                except KeyError:\n+                except (KeyError, TypeError):\n                     pass\n                 else:\n                     validated_value = unmarshal.call_and_store(\n"
  },
  {
    "instance_id": "pvlib__pvlib-python-1707",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/pvlib/iam.py b/pvlib/iam.py\n--- a/pvlib/iam.py\n+++ b/pvlib/iam.py\n@@ -175,8 +175,12 @@ def physical(aoi, n=1.526, K=4.0, L=0.002, *, n_ar=None):\n     n2costheta2 = n2 * costheta\n \n     # reflectance of s-, p-polarized, and normal light by the first interface\n-    rho12_s = ((n1costheta1 - n2costheta2) / (n1costheta1 + n2costheta2)) ** 2\n-    rho12_p = ((n1costheta2 - n2costheta1) / (n1costheta2 + n2costheta1)) ** 2\n+    with np.errstate(divide='ignore', invalid='ignore'):\n+        rho12_s = \\\n+            ((n1costheta1 - n2costheta2) / (n1costheta1 + n2costheta2)) ** 2\n+        rho12_p = \\\n+            ((n1costheta2 - n2costheta1) / (n1costheta2 + n2costheta1)) ** 2\n+\n     rho12_0 = ((n1 - n2) / (n1 + n2)) ** 2\n \n     # transmittance through the first interface\n@@ -208,13 +212,22 @@ def physical(aoi, n=1.526, K=4.0, L=0.002, *, n_ar=None):\n         tau_0 *= (1 - rho23_0) / (1 - rho23_0 * rho12_0)\n \n     # transmittance after absorption in the glass\n-    tau_s *= np.exp(-K * L / costheta)\n-    tau_p *= np.exp(-K * L / costheta)\n+    with np.errstate(divide='ignore', invalid='ignore'):\n+        tau_s *= np.exp(-K * L / costheta)\n+        tau_p *= np.exp(-K * L / costheta)\n+\n     tau_0 *= np.exp(-K * L)\n \n     # incidence angle modifier\n     iam = (tau_s + tau_p) / 2 / tau_0\n \n+    # for light coming from behind the plane, none can enter the module\n+    # when n2 > 1, this is already the case\n+    if np.isclose(n2, 1).any():\n+        iam = np.where(aoi >= 90, 0, iam)\n+        if isinstance(aoi, pd.Series):\n+            iam = pd.Series(iam, index=aoi.index)\n+\n     return iam\n \n \n"
  },
  {
    "instance_id": "pvlib__pvlib-python-1072",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/pvlib/temperature.py b/pvlib/temperature.py\n--- a/pvlib/temperature.py\n+++ b/pvlib/temperature.py\n@@ -599,8 +599,9 @@ def fuentes(poa_global, temp_air, wind_speed, noct_installed, module_height=5,\n     # n.b. the way Fuentes calculates the first timedelta makes it seem like\n     # the value doesn't matter -- rather than recreate it here, just assume\n     # it's the same as the second timedelta:\n-    timedelta_hours = np.diff(poa_global.index).astype(float) / 1e9 / 60 / 60\n-    timedelta_hours = np.append([timedelta_hours[0]], timedelta_hours)\n+    timedelta_seconds = poa_global.index.to_series().diff().dt.total_seconds()\n+    timedelta_hours = timedelta_seconds / 3600\n+    timedelta_hours.iloc[0] = timedelta_hours.iloc[1]\n \n     tamb_array = temp_air + 273.15\n     sun_array = poa_global * absorp\n"
  },
  {
    "instance_id": "pvlib__pvlib-python-1606",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/pvlib/tools.py b/pvlib/tools.py\n--- a/pvlib/tools.py\n+++ b/pvlib/tools.py\n@@ -341,6 +341,8 @@ def _golden_sect_DataFrame(params, lower, upper, func, atol=1e-8):\n     --------\n     pvlib.singlediode._pwr_optfcn\n     \"\"\"\n+    if np.any(upper - lower < 0.):\n+        raise ValueError('upper >= lower is required')\n \n     phim1 = (np.sqrt(5) - 1) / 2\n \n@@ -349,16 +351,8 @@ def _golden_sect_DataFrame(params, lower, upper, func, atol=1e-8):\n     df['VL'] = lower\n \n     converged = False\n-    iterations = 0\n \n-    # handle all NaN case gracefully\n-    with warnings.catch_warnings():\n-        warnings.filterwarnings(action='ignore',\n-                                message='All-NaN slice encountered')\n-        iterlimit = 1 + np.nanmax(\n-            np.trunc(np.log(atol / (df['VH'] - df['VL'])) / np.log(phim1)))\n-\n-    while not converged and (iterations <= iterlimit):\n+    while not converged:\n \n         phi = phim1 * (df['VH'] - df['VL'])\n         df['V1'] = df['VL'] + phi\n@@ -373,22 +367,16 @@ def _golden_sect_DataFrame(params, lower, upper, func, atol=1e-8):\n \n         err = abs(df['V2'] - df['V1'])\n \n-        # works with single value because err is np.float64\n-        converged = (err[~np.isnan(err)] < atol).all()\n-        # err will be less than atol before iterations hit the limit\n-        # but just to be safe\n-        iterations += 1\n-\n-    if iterations > iterlimit:\n-        raise Exception(\"Iterations exceeded maximum. Check that func\",\n-                        \" is not NaN in (lower, upper)\")  # pragma: no cover\n+        # handle all NaN case gracefully\n+        with warnings.catch_warnings():\n+            warnings.filterwarnings(action='ignore',\n+                                    message='All-NaN slice encountered')\n+            converged = np.all(err[~np.isnan(err)] < atol)\n \n-    try:\n-        func_result = func(df, 'V1')\n-        x = np.where(np.isnan(func_result), np.nan, df['V1'])\n-    except KeyError:\n-        func_result = np.full_like(upper, np.nan)\n-        x = func_result.copy()\n+    # best estimate of location of maximum\n+    df['max'] = 0.5 * (df['V1'] + df['V2'])\n+    func_result = func(df, 'max')\n+    x = np.where(np.isnan(func_result), np.nan, df['max'])\n \n     return func_result, x\n \n"
  },
  {
    "instance_id": "pvlib__pvlib-python-1854",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n--- a/pvlib/pvsystem.py\n+++ b/pvlib/pvsystem.py\n@@ -101,10 +101,11 @@ class PVSystem:\n \n     Parameters\n     ----------\n-    arrays : iterable of Array, optional\n-        List of arrays that are part of the system. If not specified\n-        a single array is created from the other parameters (e.g.\n-        `surface_tilt`, `surface_azimuth`). Must contain at least one Array,\n+    arrays : Array or iterable of Array, optional\n+        An Array or list of arrays that are part of the system. If not\n+        specified a single array is created from the other parameters (e.g.\n+        `surface_tilt`, `surface_azimuth`). If specified as a list, the list\n+        must contain at least one Array;\n         if length of arrays is 0 a ValueError is raised. If `arrays` is\n         specified the following PVSystem parameters are ignored:\n \n@@ -220,6 +221,8 @@ def __init__(self,\n                 strings_per_inverter,\n                 array_losses_parameters,\n             ),)\n+        elif isinstance(arrays, Array):\n+            self.arrays = (arrays,)\n         elif len(arrays) == 0:\n             raise ValueError(\"PVSystem must have at least one Array. \"\n                              \"If you want to create a PVSystem instance \"\n"
  },
  {
    "instance_id": "pvlib__pvlib-python-1154",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/pvlib/irradiance.py b/pvlib/irradiance.py\n--- a/pvlib/irradiance.py\n+++ b/pvlib/irradiance.py\n@@ -886,8 +886,9 @@ def reindl(surface_tilt, surface_azimuth, dhi, dni, ghi, dni_extra,\n     # these are the () and [] sub-terms of the second term of eqn 8\n     term1 = 1 - AI\n     term2 = 0.5 * (1 + tools.cosd(surface_tilt))\n-    term3 = 1 + np.sqrt(HB / ghi) * (tools.sind(0.5 * surface_tilt) ** 3)\n-\n+    with np.errstate(invalid='ignore', divide='ignore'):\n+        hb_to_ghi = np.where(ghi == 0, 0, np.divide(HB, ghi))\n+    term3 = 1 + np.sqrt(hb_to_ghi) * (tools.sind(0.5 * surface_tilt)**3)\n     sky_diffuse = dhi * (AI * Rb + term1 * term2 * term3)\n     sky_diffuse = np.maximum(sky_diffuse, 0)\n \n"
  },
  {
    "instance_id": "pylint-dev__astroid-1978",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/astroid/raw_building.py b/astroid/raw_building.py\n--- a/astroid/raw_building.py\n+++ b/astroid/raw_building.py\n@@ -10,11 +10,14 @@\n \n import builtins\n import inspect\n+import io\n+import logging\n import os\n import sys\n import types\n import warnings\n from collections.abc import Iterable\n+from contextlib import redirect_stderr, redirect_stdout\n from typing import Any, Union\n \n from astroid import bases, nodes\n@@ -22,6 +25,9 @@\n from astroid.manager import AstroidManager\n from astroid.nodes import node_classes\n \n+logger = logging.getLogger(__name__)\n+\n+\n _FunctionTypes = Union[\n     types.FunctionType,\n     types.MethodType,\n@@ -471,7 +477,26 @@ def imported_member(self, node, member, name: str) -> bool:\n             # check if it sounds valid and then add an import node, else use a\n             # dummy node\n             try:\n-                getattr(sys.modules[modname], name)\n+                with redirect_stderr(io.StringIO()) as stderr, redirect_stdout(\n+                    io.StringIO()\n+                ) as stdout:\n+                    getattr(sys.modules[modname], name)\n+                    stderr_value = stderr.getvalue()\n+                    if stderr_value:\n+                        logger.error(\n+                            \"Captured stderr while getting %s from %s:\\n%s\",\n+                            name,\n+                            sys.modules[modname],\n+                            stderr_value,\n+                        )\n+                    stdout_value = stdout.getvalue()\n+                    if stdout_value:\n+                        logger.info(\n+                            \"Captured stdout while getting %s from %s:\\n%s\",\n+                            name,\n+                            sys.modules[modname],\n+                            stdout_value,\n+                        )\n             except (KeyError, AttributeError):\n                 attach_dummy_node(node, name, member)\n             else:\n"
  },
  {
    "instance_id": "pylint-dev__astroid-1333",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/astroid/modutils.py b/astroid/modutils.py\n--- a/astroid/modutils.py\n+++ b/astroid/modutils.py\n@@ -297,6 +297,9 @@ def _get_relative_base_path(filename, path_to_check):\n     if os.path.normcase(real_filename).startswith(path_to_check):\n         importable_path = real_filename\n \n+    # if \"var\" in path_to_check:\n+    #     breakpoint()\n+\n     if importable_path:\n         base_path = os.path.splitext(importable_path)[0]\n         relative_base_path = base_path[len(path_to_check) :]\n@@ -307,8 +310,11 @@ def _get_relative_base_path(filename, path_to_check):\n \n def modpath_from_file_with_callback(filename, path=None, is_package_cb=None):\n     filename = os.path.expanduser(_path_from_filename(filename))\n+    paths_to_check = sys.path.copy()\n+    if path:\n+        paths_to_check += path\n     for pathname in itertools.chain(\n-        path or [], map(_cache_normalize_path, sys.path), sys.path\n+        paths_to_check, map(_cache_normalize_path, paths_to_check)\n     ):\n         if not pathname:\n             continue\n"
  },
  {
    "instance_id": "pylint-dev__astroid-1196",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/astroid/nodes/node_classes.py b/astroid/nodes/node_classes.py\n--- a/astroid/nodes/node_classes.py\n+++ b/astroid/nodes/node_classes.py\n@@ -2346,24 +2346,33 @@ def itered(self):\n         \"\"\"\n         return [key for (key, _) in self.items]\n \n-    def getitem(self, index, context=None):\n+    def getitem(\n+        self, index: Const | Slice, context: InferenceContext | None = None\n+    ) -> NodeNG:\n         \"\"\"Get an item from this node.\n \n         :param index: The node to use as a subscript index.\n-        :type index: Const or Slice\n \n         :raises AstroidTypeError: When the given index cannot be used as a\n             subscript index, or if this node is not subscriptable.\n         :raises AstroidIndexError: If the given index does not exist in the\n             dictionary.\n         \"\"\"\n+        # pylint: disable-next=import-outside-toplevel; circular import\n+        from astroid.helpers import safe_infer\n+\n         for key, value in self.items:\n             # TODO(cpopa): no support for overriding yet, {1:2, **{1: 3}}.\n             if isinstance(key, DictUnpack):\n+                inferred_value = safe_infer(value, context)\n+                if not isinstance(inferred_value, Dict):\n+                    continue\n+\n                 try:\n-                    return value.getitem(index, context)\n+                    return inferred_value.getitem(index, context)\n                 except (AstroidTypeError, AstroidIndexError):\n                     continue\n+\n             for inferredkey in key.infer(context):\n                 if inferredkey is util.Uninferable:\n                     continue\n"
  },
  {
    "instance_id": "pylint-dev__astroid-1866",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/astroid/brain/brain_builtin_inference.py b/astroid/brain/brain_builtin_inference.py\n--- a/astroid/brain/brain_builtin_inference.py\n+++ b/astroid/brain/brain_builtin_inference.py\n@@ -954,8 +954,10 @@ def _infer_str_format_call(\n \n     try:\n         formatted_string = format_template.format(*pos_values, **keyword_values)\n-    except (IndexError, KeyError):\n-        # If there is an IndexError there are too few arguments to interpolate\n+    except (IndexError, KeyError, TypeError, ValueError):\n+        # IndexError: there are too few arguments to interpolate\n+        # TypeError: Unsupported format string\n+        # ValueError: Unknown format code\n         return iter([util.Uninferable])\n \n     return iter([nodes.const_factory(formatted_string)])\n"
  },
  {
    "instance_id": "pylint-dev__astroid-1268",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/astroid/nodes/as_string.py b/astroid/nodes/as_string.py\n--- a/astroid/nodes/as_string.py\n+++ b/astroid/nodes/as_string.py\n@@ -36,6 +36,7 @@\n         MatchSingleton,\n         MatchStar,\n         MatchValue,\n+        Unknown,\n     )\n \n # pylint: disable=unused-argument\n@@ -643,6 +644,9 @@ def visit_property(self, node):\n     def visit_evaluatedobject(self, node):\n         return node.original.accept(self)\n \n+    def visit_unknown(self, node: \"Unknown\") -> str:\n+        return str(node)\n+\n \n def _import_string(names):\n     \"\"\"return a list of (name, asname) formatted as a string\"\"\"\n"
  },
  {
    "instance_id": "pyvista__pyvista-4315",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/pyvista/core/grid.py b/pyvista/core/grid.py\n--- a/pyvista/core/grid.py\n+++ b/pyvista/core/grid.py\n@@ -135,23 +135,30 @@ def __init__(self, *args, check_duplicates=False, deep=False, **kwargs):\n                     self.shallow_copy(args[0])\n             elif isinstance(args[0], (str, pathlib.Path)):\n                 self._from_file(args[0], **kwargs)\n-            elif isinstance(args[0], np.ndarray):\n-                self._from_arrays(args[0], None, None, check_duplicates)\n+            elif isinstance(args[0], (np.ndarray, Sequence)):\n+                self._from_arrays(np.asanyarray(args[0]), None, None, check_duplicates)\n             else:\n                 raise TypeError(f'Type ({type(args[0])}) not understood by `RectilinearGrid`')\n \n         elif len(args) == 3 or len(args) == 2:\n-            arg0_is_arr = isinstance(args[0], np.ndarray)\n-            arg1_is_arr = isinstance(args[1], np.ndarray)\n+            arg0_is_arr = isinstance(args[0], (np.ndarray, Sequence))\n+            arg1_is_arr = isinstance(args[1], (np.ndarray, Sequence))\n             if len(args) == 3:\n-                arg2_is_arr = isinstance(args[2], np.ndarray)\n+                arg2_is_arr = isinstance(args[2], (np.ndarray, Sequence))\n             else:\n                 arg2_is_arr = False\n \n             if all([arg0_is_arr, arg1_is_arr, arg2_is_arr]):\n-                self._from_arrays(args[0], args[1], args[2], check_duplicates)\n+                self._from_arrays(\n+                    np.asanyarray(args[0]),\n+                    np.asanyarray(args[1]),\n+                    np.asanyarray(args[2]),\n+                    check_duplicates,\n+                )\n             elif all([arg0_is_arr, arg1_is_arr]):\n-                self._from_arrays(args[0], args[1], None, check_duplicates)\n+                self._from_arrays(\n+                    np.asanyarray(args[0]), np.asanyarray(args[1]), None, check_duplicates\n+                )\n             else:\n                 raise TypeError(\"Arguments not understood by `RectilinearGrid`.\")\n \n"
  },
  {
    "instance_id": "pydicom__pydicom-1694",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/pydicom/dataset.py b/pydicom/dataset.py\n--- a/pydicom/dataset.py\n+++ b/pydicom/dataset.py\n@@ -2492,8 +2492,8 @@ def to_json_dict(\n         json_dataset = {}\n         for key in self.keys():\n             json_key = '{:08X}'.format(key)\n-            data_element = self[key]\n             try:\n+                data_element = self[key]\n                 json_dataset[json_key] = data_element.to_json_dict(\n                     bulk_data_element_handler=bulk_data_element_handler,\n                     bulk_data_threshold=bulk_data_threshold\n"
  },
  {
    "instance_id": "pydicom__pydicom-1413",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/pydicom/dataelem.py b/pydicom/dataelem.py\n--- a/pydicom/dataelem.py\n+++ b/pydicom/dataelem.py\n@@ -433,13 +433,24 @@ def value(self) -> Any:\n     @value.setter\n     def value(self, val: Any) -> None:\n         \"\"\"Convert (if necessary) and set the value of the element.\"\"\"\n+        # Ignore backslash characters in these VRs, based on:\n+        # * Which str VRs can have backslashes in Part 5, Section 6.2\n+        # * All byte VRs\n+        exclusions = [\n+            'LT', 'OB', 'OD', 'OF', 'OL', 'OV', 'OW', 'ST', 'UN', 'UT',\n+            'OB/OW', 'OW/OB', 'OB or OW', 'OW or OB',\n+            # Probably not needed\n+            'AT', 'FD', 'FL', 'SQ', 'SS', 'SL', 'UL',\n+        ]\n+\n         # Check if is a string with multiple values separated by '\\'\n         # If so, turn them into a list of separate strings\n         #  Last condition covers 'US or SS' etc\n-        if isinstance(val, (str, bytes)) and self.VR not in \\\n-                ['UT', 'ST', 'LT', 'FL', 'FD', 'AT', 'OB', 'OW', 'OF', 'SL',\n-                 'SQ', 'SS', 'UL', 'OB/OW', 'OW/OB', 'OB or OW',\n-                 'OW or OB', 'UN'] and 'US' not in self.VR:\n+        if (\n+            isinstance(val, (str, bytes))\n+            and self.VR not in exclusions\n+            and 'US' not in self.VR\n+        ):\n             try:\n                 if _backslash_str in val:\n                     val = cast(str, val).split(_backslash_str)\n"
  },
  {
    "instance_id": "pydicom__pydicom-901",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/pydicom/config.py b/pydicom/config.py\n--- a/pydicom/config.py\n+++ b/pydicom/config.py\n@@ -62,10 +62,7 @@ def DS_decimal(use_Decimal_boolean=True):\n \n # Logging system and debug function to change logging level\n logger = logging.getLogger('pydicom')\n-handler = logging.StreamHandler()\n-formatter = logging.Formatter(\"%(message)s\")\n-handler.setFormatter(formatter)\n-logger.addHandler(handler)\n+logger.addHandler(logging.NullHandler())\n \n \n import pydicom.pixel_data_handlers.numpy_handler as np_handler  # noqa\n@@ -110,16 +107,29 @@ def get_pixeldata(ds):\n \"\"\"\n \n \n-def debug(debug_on=True):\n-    \"\"\"Turn debugging of DICOM file reading and writing on or off.\n+def debug(debug_on=True, default_handler=True):\n+    \"\"\"Turn on/off debugging of DICOM file reading and writing.\n+\n     When debugging is on, file location and details about the\n     elements read at that location are logged to the 'pydicom'\n     logger using python's logging module.\n \n-    :param debug_on: True (default) to turn on debugging,\n-    False to turn off.\n+    Parameters\n+    ----------\n+    debug_on : bool, optional\n+        If True (default) then turn on debugging, False to turn off.\n+    default_handler : bool, optional\n+        If True (default) then use ``logging.StreamHandler()`` as the handler\n+        for log messages.\n     \"\"\"\n     global logger, debugging\n+\n+    if default_handler:\n+        handler = logging.StreamHandler()\n+        formatter = logging.Formatter(\"%(message)s\")\n+        handler.setFormatter(formatter)\n+        logger.addHandler(handler)\n+\n     if debug_on:\n         logger.setLevel(logging.DEBUG)\n         debugging = True\n@@ -129,4 +139,4 @@ def debug(debug_on=True):\n \n \n # force level=WARNING, in case logging default is set differently (issue 103)\n-debug(False)\n+debug(False, False)\n"
  },
  {
    "instance_id": "pydicom__pydicom-1139",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/pydicom/valuerep.py b/pydicom/valuerep.py\n--- a/pydicom/valuerep.py\n+++ b/pydicom/valuerep.py\n@@ -1,6 +1,5 @@\n # Copyright 2008-2018 pydicom authors. See LICENSE file for details.\n \"\"\"Special classes for DICOM value representations (VR)\"\"\"\n-from copy import deepcopy\n from decimal import Decimal\n import re\n \n@@ -750,6 +749,25 @@ def __ne__(self, other):\n     def __str__(self):\n         return '='.join(self.components).__str__()\n \n+    def __next__(self):\n+        # Get next character or stop iteration\n+        if self._i < self._rep_len:\n+            c = self._str_rep[self._i]\n+            self._i += 1\n+            return c\n+        else:\n+            raise StopIteration\n+\n+    def __iter__(self):\n+        # Get string rep. and length, initialize index counter\n+        self._str_rep = self.__str__()\n+        self._rep_len = len(self._str_rep)\n+        self._i = 0\n+        return self\n+\n+    def __contains__(self, x):\n+        return x in self.__str__()\n+\n     def __repr__(self):\n         return '='.join(self.components).__repr__()\n \n"
  },
  {
    "instance_id": "pydicom__pydicom-1256",
    "model_name_or_path": "gpt-4o",
    "model_patch": "diff --git a/pydicom/jsonrep.py b/pydicom/jsonrep.py\n--- a/pydicom/jsonrep.py\n+++ b/pydicom/jsonrep.py\n@@ -226,7 +226,8 @@ def get_sequence_item(self, value):\n                     value_key = unique_value_keys[0]\n                     elem = DataElement.from_json(\n                         self.dataset_class, key, vr,\n-                        val[value_key], value_key\n+                        val[value_key], value_key,\n+                        self.bulk_data_element_handler\n                     )\n                 ds.add(elem)\n         return ds\n"
  }
]